{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: FMRI Data and First-Level Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll be introduced to fMRI analysis in Python and we will perform a first-level analysis. As with most statistical fields, there are many ways to analyze data in fMRI. Here, once we have covered some basics, we will introduce you to one of the most commonplace approaches; the mass-univariate linear model and use it to run a first-level analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    " - [Notebook 0: Introduction](./nb_00_introduction.ipynb)\n",
    " - [**Notebook 1: FMRI Data and First-Level Analysis**](./nb_01_fmri_data_and_first_level_analysis.ipynb)\n",
    "   - [Getting Started](#Getting-Started)\n",
    "     - [Background: What is fMRI?](#Background:-What-is-fMRI?)\n",
    "     - [Loading the Data](#Loading-the-Data)\n",
    "     - [The NIfTI and img file formats](#The-NIfTI-and-img-file-formats)\n",
    "     - [Preprocessing](#Preprocessing)\n",
    "     - [The Linear Model for Imaging Data](#The-Linear-Model-for-Imaging-Data)\n",
    "     - [First-Level vs Second-Level Analysis](#First-Level-vs-Second-Level-Analysis)\n",
    "   - [First-level Analysis: An example](#First-level-Analysis:-An-example)\n",
    "     - [Building $X$ and Fitting the Model](#Building-$X$-and-Fitting-the-Model)\n",
    "     - [Creating a Contrast Vector](#Creating-a-Contrast-Vector)\n",
    "     - [Computing Effect Sizes](#Computing-Effect-Sizes)\n",
    "     - [Computing $Z$- and $t$-Statistic Maps](#Computing-$Z$--and-$t$-Statistic-Maps)\n",
    "     - [Thresholding your Results](#Thresholding-your-Results)\n",
    "     - [$F$-Tests](#$F$-Tests)\n",
    "   - [Exercises](#Exercises)\n",
    "     \n",
    "     \n",
    " - [Notebook 2: Second-Level Analysis](./nb_02_second_level_analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's load in the `nibabel` (we shall use this to manipulate `nifti` and `img` files) and `nilearn` packages which we shall use to perform statistical analysis. The `nilearn` package makes heavy use of `matplotlib`, and so we will need to make sure we have matplotlib configured correctly (this is what the `%matplotlib inline` command is doing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install nilearn==0.10.1\n",
    "%matplotlib inline\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background: What is fMRI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the data, let's give a brief overview of fMRI. To understand what fMRI measures, we must first describe the anatomy of a neuron. The human brain is comprised of approximately $8.6 \\times 10^{10}$ neurons, which come in a wide variety of shapes, sizes and functions. Broadly speaking, neurons are comprised of three parts;\n",
    "\n",
    " - The cell body (soma)\n",
    " - The dendrites\n",
    " - An axon. \n",
    " \n",
    "Neurons are connected to one another via **synapses**: junctions, or gaps, which lie between the axon terminal (the tip of a small branch at the end of an axon) of one neuron and the dendrite of another. Each neuron shares a synapse with approximately $7,000$ neighbouring neurons, and it is through each of these synaptic connections that the electro-chemical communication, which is believed to represent cognitive function, occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a figure of some neurons! Run this code!\n",
    "from IPython.display import Image\n",
    "Image(os.path.join('Figs','Neurons.png'),width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons communicate using electrical impulses known as action potentials. Triggered by complex processes in the soma, an action potential travels along the axon until it reaches an axon terminal, where it then encounters a synapse. At the synapse, the electrical impulse is converted into chemical form via the use of chemical \"neurotransmitters\" and other cellular processes, secreted across the synapse and received by a dendrite in the neighbouring neuron. Once the neurotransmitters arrive at the dendrite, the signal is propagated to the neighbouring cell's soma. Inside the soma of the neighbouring cell, the signal may then stimulate or inhibit the firing of action potentials, and the process repeats itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transmission of chemicals across synapses is a process known as neuronal coupling and requires large amounts of energy, which is provided by oxygenated haemoglobin in red blood cells. The magnetic properties of haemoglobin differ depending on whether the haemoglobin is carrying oxygen or not, and it is these differences in magnetism that may be detected by an fMRI scanner. To be specific, deoxygenated haemoglobin is more magnetic than oxygenated haemoglobin and induces inhomogeneities in the local magnetic field created by an fMRI scanner. These inhomogeneities are measured by the fMRI scanner and recorded as the **Blood Oxygen Level Dependent (BOLD)** response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, lets load in some example data! The example we will be running through in this notebook is borrowed from the [single-session, single-subject fMRI example from the nilearn website](https://nilearn.github.io/dev/auto_examples/00_tutorials/plot_single_subject_single_run.html) but was originally collected by the [FIL methods group which develops the SPM software](https://www.fil.ion.ucl.ac.uk/spm/data/auditory/). \n",
    "\n",
    "Rather handily, the nilearn package includes this data as a freely available example which can be retreived with the `fetch_spm_auditory` function in the `datasets` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.datasets import fetch_spm_auditory\n",
    "subject_data = fetch_spm_auditory(data_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create this dataset, 100 scans were acquired from a single subject during a single session using a repetition time (TR) of 7 seconds (we have one scan per 7 seconds). The first four scans have been removed during preprocessing.\n",
    "\n",
    "The subject underwent alternating periods of stimulation and rest, lasting 42s each (that is, for 6 scans). The stimulation consisted of bi-syllabic words being presented binaurally (through audio, to both ears) at a rate of 60 per minute.  \n",
    "\n",
    "Let's have a look at what is in the `subject_data` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as can be seen the `subject_data` object is a dict containing $3$ keys; `func`, `anat` and `events`. Let's go through each of these and see what they are;\n",
    "\n",
    " - `func`: This is the subject's functional data (i.e. the fMRI images representing [BOLD response](https://en.wikipedia.org/wiki/Blood-oxygen-level-dependent_imaging)). There are 96 of these images.\n",
    " - `anat`: This is the subject's anatomical data (i.e. an image of their physical anatomy, taken for reference). There is only one anatomical image.\n",
    " - `events`: This file contains information about the experimental design (i.e. details on when the stimulated and rest periods were)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Note:** In general, there is no agreed-upon standard in the imaging literature for naming and storing the results of an imaging session (although there are proposed approaches; c.f. [BIDS and NIDM](https://www.incf.org/blog/bids-and-neuroimaging-data-model-nidm)). However, in most cases you will find that the functional data is named something along the lines of `functional` and the anatomical is named something along the lines of `anatomical` or `structural`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in a functional image and the anatomical to have a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_stat_map, plot_anat, plot_img\n",
    "\n",
    "# Plot the 11th (remember zero-indexing!) functional image\n",
    "plot_img(subject_data.func[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the subjects anatomical data\n",
    "plot_anat(subject_data.anat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the `plot_img` [documentation](https://nilearn.github.io/dev/modules/generated/nilearn.plotting.plot_img.html) to see if you can get the above images to line up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a brief look at the event's file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "events = pd.read_csv(subject_data['events'],sep='\\t')\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it tells us the `onset` of each \"block\" in the design (e.g. the time the block began), the `duration` of the block, and the `trial_type` of the block (i.e. what was happening during the block? Was the subject active or at rest?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NIfTI and img file formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to some statistics analysis, it's worth just providing a little bit of background on the `img` file format and what is stored inside it. Let's try loading an `img` file in. \n",
    "\n",
    "We can do this using the nibabel `load` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nib.load(os.path.join('spm_auditory','sub001','fM00223','fM00223_099.img'))\n",
    "\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print this `img` object we get a load of metadata! Let's highlight a few notable things here;\n",
    "\n",
    " - We have the shape of the data; `(64,64,64)`. \n",
    " \n",
    "   This means that in each image we have $64\\times 64\\times 64=262144$ voxels (3D pixels). Typically, the brain takes up approximately a third of the image so this means we probably have about $262144/3\\approx 87381$ datapoints *per image!* (and these are small images!).\n",
    "   \n",
    "   \n",
    " - We have the `affine` of the data. This tells us the position of the image array data in a reference space. In other words, during data collection and preprocessing, this data may have undergone a number of transformations to make all of the images \"look nice\" and \"line up\" with one another. This `affine` tells us how we can transform our version of the image back to the original frame of reference.\n",
    " \n",
    " \n",
    " - We have the `nibabel.nifti1.Nifti1PairHeader` object. This `header` object stores all sorts of useful information about the data. We won't go into too much detail on the header here but there's a very good blog post [here](https://brainder.org/2012/09/23/the-nifti-file-format/) if you wanted to read a bit more.\n",
    " \n",
    "You might be wondering at this point \"where's the image data?\". Well, `img` is an `nibabel.nifti1` object and `nibabel` allows us to access information inside `nibabel.nifti1` objects using the `dataobj` property like so;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "img_data = img.dataobj\n",
    "\n",
    "# Print the data shape\n",
    "print(img_data.shape)\n",
    "\n",
    "# Try accessing some elements of the data\n",
    "print(img_data[10:12,50:54,30:35])\n",
    "\n",
    "# Print the type of the data \n",
    "print(type(img_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manipulate and use the `img_data` object much like we would a numpy array! In fact, you may be thinking \"why don't we just use a numpy array instead of whatever a `nibabel.arrayproxy.ArrayProxy` is?\".\n",
    "\n",
    "The short answer to this question is; memory. Neuroimaging data is large. Very large. If you were to try and load all the data in this example in as numpy arrays you would likely run into a memory error and crash the notebook. And this example is small.\n",
    "\n",
    "Nibabel is a very powerful tool as it handles a lot of these memory issues for you behind the scenes. Instead of creating a numpy array, nibabel has created something called a `memmap` here, which basically only loads in data when you ask for it! For instance, in the above example the elements `[10:12,50:54,30:35]` weren't read into working memory until we explicitly asked for them.\n",
    "\n",
    " > **Tip:** If you're working with neuroimaging data and everything is running slowly, check if you have your data saved as a `memmap` or as a `np.array`. It could save you years of your life!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one final piece of data storage related information worth noting. If we look inside the `nilearn_data` folder, we can see that every `img` file has a corresponding `hdr` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(os.path.join('spm_auditory','sub001','fM00223')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we move the `hdr` file, weirdly we can't use the `img` file anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filename of a hdr file and make a temporary filename \n",
    "hdr_file = os.path.join('spm_auditory','sub001','fM00223','fM00223_099.hdr')\n",
    "tmp_file = os.path.join('spm_auditory','sub001','fM00223','tmp.hdr')\n",
    "\n",
    "# Move the hdr file\n",
    "os.rename(hdr_file, tmp_file)\n",
    "\n",
    "# Try to load in the img file now\n",
    "img = nib.load(os.path.join('spm_auditory','sub001','fM00223','fM00223_099.img'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we move the `hdr` file back in place, we can load in the `img` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(tmp_file, hdr_file)\n",
    "img3 = nib.load(os.path.join('spm_auditory','sub001','fM00223','fM00223_099.img'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit annoying... this is happening because `img` and `hdr` files come in pairs. The `img` file contains the data and the `hdr` file contains the header meta-data we were looking at earlier.\n",
    "\n",
    "Because this is a bit of a pain, people often use a \"Neuroimaging Informatics Technology Initiative\" (NIfTI) file format instead. Simply put, a NIfTI file is just combination of the information inside the `img` and `hdr` files, saved under a `.nii` extension.\n",
    "\n",
    "`NIfTI` files are popular and often the default file extension used in neuroimaging, so you may find that as you work through this tutorial a lot of the results we are saving are saved as `.nii` rather than `.img`. Just remember that the `.nii` file is basically just the same thing as \"`.img`+`.hdr`\". Here's a quick example;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import mean_img\n",
    "\n",
    "# Lets make a mean image of the subjects functional data\n",
    "mean_img = mean_img(subject_data.func)\n",
    "\n",
    "# Lets save the result \n",
    "nib.save(mean_img, os.path.join('spm_auditory','sub001','mean_image'))\n",
    "\n",
    "# Let's see what it saved it as:\n",
    "print(os.listdir(os.path.join('spm_auditory','sub001')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a `mean_image.nii` file. It is worth stressing that all of the tools we use in this notebook will work for both `.img` and `.nii` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've never worked with imaging data before you might at this point be thinking something along the lines of \"hey, don't people move in the scanner though?\" or \"surely, we're going to have to do something else to make these images line up?\". Normally, you would address such questions when you preprocess your data. But for our purposes today, this has already been done for you. \n",
    "\n",
    "That said, preprocessing is an important part of any imaging study and it is important to know what has happened to the data before analysis, so that you can understand what the analysis is taking care of. Here we'll just briefly list a few common preprocessing steps which may have been applied to the data before analysis:\n",
    "\n",
    "- **Distortion Correction:** Functional MRI images are generated using a magnetic field induced by a superconducting coil. In theory, the magnetic field should be perfectly homogeneous and uniform across space. However, in practice, inhomogeneities in the magnetic field are common, often caused by differences in the magnetic susceptibility properties of the tissue types being scanned. Fortunately, magnetic field inhomogeneties behave in a predictable manner and are often fixed in a preprocessing stage by \"warping\" an image.\n",
    "    \n",
    "    \n",
    "- **Motion Correction:** Image acquisition can be a lengthy process and it is difficult for participants to remain completely still for such prolonged periods of time. As a result, it is inevitable that there will be some movement of the head during any image acquisition. In this stage of the pre-processing pipeline, images are transformed to line up with one another (often the middle image of the time series is used as a reference).\n",
    "\n",
    "\n",
    "- **Co-registration:** In order to compare them to something meaningful, we usually want our images to \"line-up\" or be \"aligned\" to the subject's anatomy. For this reason, the images are often \"co-registered\" to the subject's anatomical image (e.g. `sM00223_002.img`) during pre-processing. This means that all of the images have now been transformed to have the same shape and size as the subject's anatomical image.\n",
    "\n",
    "\n",
    "- **Registration:** Often, we are also interested in comparing the results of analysis \"between-subjects\". For this reason, images are often aligned to some \"standard template\" to allow between subject comparison. We will revisit this idea in the next notebook.\n",
    "\n",
    "\n",
    "- **Brain Extraction:** Non-brain structure, such as the skull and surrounding tissues, are typically removed from all of the images before analysis. This process is performed using the anatomical image as a reference and is also sometimes known as skull-stripping.\n",
    "\n",
    "\n",
    "- **Spatial Smoothing:** Spatial data is noisy. One thing we can do to combat this is convolve the data with a Gaussian kernel along each of the three dimensions (i.e. make it *smoother*). This is often done during pre-processing but it is also common to perform smoothing during the first-level analysis. We will investigate this further in the first-level example in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list is non-exhaustive but it does highlight something important about fMRI analysis. Before beginning, statistical analysis, you must always **LOOK AT YOUR DATA!**\n",
    "\n",
    "Any one of these preprocessing steps could go wrong, and, more often than not, they do!\n",
    "\n",
    "In the box below have a look through the functional images in `subject_data` using the `plot_img` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what it might look like if each of the above preprocessing steps had failed. When you are happy you have looked carefully at the images, move on to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Linear Model for Imaging Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we're almost ready to start running code! We will just quickly go over a few statistical concepts and then we will move onto running an analysis! One of the most common approaches employed for analyzing fMRI data is to use the Linear Model (LM). Below, we'll give a brief recap of what the LM looks like;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the traditional (univariate) settings you are likely familiar with, a LM containing $n$ observations typically takes the following form:\n",
    "\n",
    "$$Y=X\\beta + \\epsilon$$ \n",
    "\n",
    "where $\\epsilon \\sim N(0,\\Sigma)$. In this model;\n",
    "\n",
    " - $Y$ is an $(n \\times 1)$-shaped response vector; i.e. some variable which we have several measurements for and are interested in modelling.\n",
    " - $X$ is an $(n \\times p)$-shaped design matrix. $X$ typically contains information about the study you have run (e.g. variables describing the experiment setup, the participants involved,... etc).\n",
    " - $\\beta$ is a $(p \\times 1)$-shaped parameter vector. The parameter vector tells us how the columns of $X$ (known as the regressors or independent variables) interact with the response vector $Y$.\n",
    " - $\\epsilon$ is an $(n \\times 1)$-shaped error term representing noise.\n",
    " - $\\Sigma$ is the $(n \\times n)$ covariance matrix of $\\epsilon$. Often, we assume that $\\Sigma$ takes some special form to save us from having to deal with $n^2$ covariance terms. For example, in the analysis we will run shortly, we take $\\Sigma$ to have an AR(1) structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A figure of a voxel time series.\n",
    "Image(os.path.join('Figs','voxTimeSeries.png'),width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to imaging, it can become a bit confusing what exactly $X$ and $Y$ are. In imaging, we do not have a single response vector $Y$, but rather we have a response vector for *each voxel* (i.e. each location in the brain). This means that, essentially, we don't just have one model, but rather thousands of models, each one corresponding to a different location in the brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $v^{th}$ voxel in the image, we have a model that looks something like this;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y^v = X \\beta^v + \\epsilon^v$$\n",
    "where $\\epsilon^v \\sim N(0,\\Sigma^v)$.  In this model;\n",
    "\n",
    " - $Y^v$ is an $(n \\times 1)$-shaped response vector representing the response *at voxel $v$*.\n",
    " - $X$ still the $(n \\times p)$-shaped design matrix.\n",
    " - $\\beta^v$ is a $(p \\times 1)$-shaped parameter vector modelling the parameters *for voxel $v$*.\n",
    " - $\\epsilon^v$ is an $(n \\times 1)$-shaped error term representing noise *at voxel $v$*.\n",
    " - $\\Sigma^v$ is the $(n \\times n)$ covariance matrix of $\\epsilon^v$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model is often referred to as a *mass-univariate* Linear Model, as each voxel has it's own seperate univariate model (and there are a lot of voxels!)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** You may be wondering why we have so many univariate models, instead of just combining everything together into a single multivariate model. Simply put, a multivariate model would be massively overparameterized. In this example, we have $n=96$ observations. If we tried to model this using a multivariate model we would have $pV$ parameters to estimate, where $p$ is the number of columns of $X$ (which is $16$ in the following example) and $V$ is the number of voxels in the brain (which we saw earlier was approximately $87381$). \n",
    ">\n",
    "> You could certainly try using your favourite approach to estimating overparameterized models to try to get something meaningful but given how small $n$ ($=96$) is in comparison to $pV$ ($=1398096$), it certainly isn't looking likely you will get anything of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth emphasizing that $X$ does not have a superscript $v$. This is because $X$ contains information about the experimental design (e.g. from the `events.tsv` file) and this information does not depend on which voxel we are looking at!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Some analyses may include voxel-specific information in $X$, but we won't get into that here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a mass-univariate analysis, we are running a lot of statistical tests (one for each voxel in the brain), and therefore a multiple comparisons correction must be employed once we have run our analysis. We'll talk about this a little bit more after we have generated some statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-Level vs Second-Level Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to explain before moving onto running the example is the difference between a \"first\" and \"second\" level analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-Level Analysis\n",
    "\n",
    "In fMRI, a first-level analysis, also known as a \"subject-level\" analysis, usually involves combining data drawn from a single subject. The subject has typically performed some task, and we have one scan per timepoint during the task.\n",
    "\n",
    "The aim of a first-level analysis is to combine several scans taken from the same subject, each of which corresponds to a timepoint, to perform some inference which assess *that subjects* response to the stimuli. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A first level analysis (for subject j with t_j timepoints)! Run this code!\n",
    "Image(os.path.join('Figs','glm.png'),width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second-Level Analysis\n",
    "\n",
    "A second-level analysis, also known as a \"group-level\" analysis, usually involves combining results from several first-level analyses. All subjects have performed the same task, the same first-level analysis has been run for each subject, and, from this, we now have one scan per subject.\n",
    "\n",
    "The aim of a second-level analysis is to combine the results of several first-level analyses, each of which corresponds to a subject, to perform some inference which assess *the groups* response to the stimuli. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A second level analysis (the capital G in the notation stands for \"Group\")!\n",
    "Image(os.path.join('Figs','glm2.png'),width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, a first-level analysis combines *timepoints for a subject* and a second-level analysis combines *subjects in a group*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Again, if you are statistically minded you might be thinking, \"why are these models run seperately? Couldn't they be combined into some sort of mixed model or something?\". The answer here is; yes, often this is done. \n",
    "> \n",
    "> However, equally often there simply isn't enough data to accurately model all of the covariance terms such a model would require (remember we only have $n=96$!). There has been a whole literature arguing for and against the \"two-stage\" approach described above (c.f. [Friston, et al. (2005)](https://pubmed.ncbi.nlm.nih.gov/15588616/), [Worsley, et al. (2002)](https://pubmed.ncbi.nlm.nih.gov/11771969/)). Regardless, it is still one of the most popular approaches employed today. \n",
    "> \n",
    "> Another important factor is computation time, but that's a whole other discussion!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-level Analysis: An example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, finally! Let's get onto running a first-level analysis. To begin with let's build the design matrix, $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building $X$ and Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nilearn contains a `first_level` module for running the linear model on subject-level data. To use it the first thing we have to do is initialize a new `FirstLevelModel` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "\n",
    "# Initialize the first level model\n",
    "fmri_glm = FirstLevelModel(t_r=7,\n",
    "                           noise_model='ar1',\n",
    "                           standardize=False,\n",
    "                           hrf_model='spm',\n",
    "                           drift_model='cosine',\n",
    "                           high_pass=.01)\n",
    "\n",
    "print(fmri_glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we have initialised a new `FirstLevelModel` object called `fmri_glm`. At the moment `fmri_glm` only contains some very basic information about the analysis we wish to run. In the above code, we have told the `fmri_glm` the following information;\n",
    "\n",
    " - `t_r`: The time of repetition (between scans) in seconds.\n",
    " \n",
    " \n",
    " - `noise_model`: The model we are assuming for the noise. In this case, we have selected an [AR($1$) model](https://en.wikipedia.org/wiki/Autoregressive_model) for $\\Sigma^v$.\n",
    " \n",
    " \n",
    " - `standardize`: This setting controls whether the time series are centered and normalized. As we have included an intercept in the design matrix (the last column of $X$) we do not need centering. As normalizing is also not necessary here we have set this to `False`.\n",
    " \n",
    " \n",
    " - `hrf_model`: In reality, we don't expect the brain to just turn \"on\" and \"off\" like a switch when neuronal activity occurs. The biology of neuronal activity is complex and must be modelled using the [Heamodynamic Response Function](https://en.wikipedia.org/wiki/Haemodynamic_response#:~:text=Haemodynamic%20response%20(HR)%20allows%20the,blood%20to%20active%20neuronal%20tissues.&text=Since%20higher%20processes%20in%20the,other%20cells%20of%20the%20brain.). In a very loose sense you can think of the Heamodynamic Response Function as a model of how a part of the brain \"warms up\" and \"starts working\" when need to start using it. There are several versions of the HRF model we could use during analysis but in general the `spm` one is fairly standard.\n",
    " \n",
    " \n",
    " - `drift_model`: This parameter tells us how we want to model drift in our timeseries data. The drift model is a set of slow oscillating functions (in our case Discrete Cosine transform) with a cut-off frequency. We use this model to represent and remove spurious low-frequency effects related to heart rate, breathing and slow drifts in the scanner signal, which we could do by setting the cutoff frequency to 1/128 Hz ~ 0.01Hz. Often we set a cutoff period as the longest period between two trials of the same condition multiplied by 2. For instance, in our case the longest period is 42s, so the `high_pass` frequency shall be 1/84 Hz ~ 0.012 Hz. This has the effect of removing periodic behaviour which is too long to be caused by our experiment. [Here](https://www.brainvoyager.com/bvqx/doc/UsersGuide/Preprocessing/TemporalHighPassFiltering.html) is a good article if you wanted to read more about this.\n",
    " \n",
    " \n",
    " \n",
    " - `high_pass`: This is the cutoff period mentioned above. In this case we have gone for 0.01Hz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have set up a `FirstLevelModel` object, we have to pass it some data. To do this we must concatenate all of the functional data into one 4D volume and pass it to the `fmri_glm.fit` method, alongside the `events.tsv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import concat_imgs\n",
    "\n",
    "# Join all the 3d functions images together into one 4d volume\n",
    "fmri_img = concat_imgs(subject_data.func)\n",
    "\n",
    "# Fit the linear model by passing the images and experimental design information to nilearn.\n",
    "fmri_glm = fmri_glm.fit(fmri_img, events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, here we are essentially giving `nilearn` the information it needs to construct $Y^v$ for every voxel $v$ and to construct $X$. `nilearn` is then using this information to construct the linear model and perform parameter estimation for each voxel in the brain (i.e. `nilearn` is estimating $\\beta^v$ and $\\Sigma^v$ for every voxel $v$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start looking at the model fit and results, let's first look at the design matrix $X$, which nilearn has constructed. We can get this using the `design_matrices_` property of `fmri_glm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix = fmri_glm.design_matrices_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make an image representing the design matrix in nilearn by using the `plot_design_matrix` function from `nilearn.plotting`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_design_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create design matrix plot\n",
    "plot_design_matrix(design_matrix)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above image the colors correspond to the numerical values in the design matrix. If your unsure what the above image means, try comparing it to the design in numerical form given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, let's quickly save the design matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "outdir = 'results'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "from os.path import join\n",
    "plot_design_matrix(\n",
    "    design_matrix, output_file=join(outdir, 'design_matrix.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in this design, we had regular periods of activity and rest which were alternated in intervals of $42$ seconds. If we let `1` represent activity and `0` represent rest we might expect that this has been coded into our linear model as something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in example file\n",
    "boxcar = pd.read_csv(os.path.join('boxcar.csv'),index_col=0)\n",
    "\n",
    "plt.plot(boxcar)\n",
    "plt.xlabel('scan')\n",
    "plt.title('Expected Auditory Response')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we look at our design matrix, the first column corresponds to the `active` periods but it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(design_matrix['active'])\n",
    "plt.xlabel('scan')\n",
    "plt.title('Expected Auditory Response')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening here? Well, as we mentioned before the brain doesn't just switch \"on\" and \"off\" in a binary fashion. \n",
    "\n",
    "When you engage some part of the brain in cognitive function, it requires oxygen and glucose, which are transported in the blood. Your arteries are able to transport this oxygen and glucose to the required brain area very quickly but they do it in an inefficient way. At first, they overshoot a little and send a bit too much (hence the peaks in the above image). After this, the process levels out and the arteries provide the required amount of oxygen and glucose (hence the flat regions after each peak). Finally, when they are no longer required to do so they stop sending blood and glucose, but again they do this in a slightly inefficient manner and undershoot (hence the dips below after each period of activity). What we are describing here is the Heamodynamic Response Function (HRF) which we mentioned earlier when we set up the `FirstLevelModel` object (the BOLD response in the figure below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The complex factors that affect the shape of the Heamodynamic (BOLD) response function.\n",
    "Image(os.path.join('Figs','bold.png'),width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Interpreting the BOLD response:** There are many nuances to the interpretation of the BOLD HRF, as the signal detected by BOLD is mediated by an array of secondary factors related to localized changes in blood flow and blood oxygenation (shown above). These factors include; \n",
    " > - The spiking activity of individual neurons \n",
    " > - Cerebral Blood Volume (CBV; the volume of blood present in a given amount of brain tissue)\n",
    " > - Cerebral Blood Flow (CBF; the rate at which blood is supplied)\n",
    " > - Cerebral Metabolic Rate of Oxygen (CMRO$_2$; the rate at which oxygen is consumed by the brain). \n",
    " >\n",
    " > However, it is generally agreed that the BOLD response serves as a reasonable proxy for neuronal activity. The above figure is based on the work of [Buxton et al. (2004)](https://pubmed.ncbi.nlm.nih.gov/15501093/) and [Buxton (2012)](https://pubmed.ncbi.nlm.nih.gov/22245339/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In other words, the above graph isn't just a model of our experimental design, it is a model of how we expect the brain to be reacting to the experimental design. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the same behaviour in the second column of our design matrix $X$, which describes what we might expect brain regions to be doing if they were responding to the `rest` periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(design_matrix['rest'])\n",
    "plt.xlabel('scan')\n",
    "plt.title('Expected Rest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mathematicaally speaking, the above images are creating by [convolving](https://en.wikipedia.org/wiki/Convolution) the binary graph two boxes above with a predefined form of the HRF (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution with the BOLD response.\n",
    "Image(os.path.join('Figs','experiment_design.png'),width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first two columns we have our cosine bases, which handle [drift](https://www.brainvoyager.com/bvqx/doc/UsersGuide/Preprocessing/TemporalHighPassFiltering.html) in the data. Here's an example of two of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('scan')\n",
    "plt.title('Long term drift')\n",
    "plt.plot(design_matrix['drift_1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('scan')\n",
    "plt.title('Shorter term drift')\n",
    "plt.plot(design_matrix['drift_9'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how in both of these images the period of oscillation of the `cosine` functions is much larger than in our `active` and `rest` columns. For this reason, if we saw any behaviour like this in the response at a voxel we would expect that such behaviour is nothing to do with our experimental design but is instead related to other physiological factors like heart rate, breathing and slow drifts in the scanner signal.\n",
    "\n",
    "Including these oscillating functions as columns in $X$ allows us to pick apart which periodic behaviours in our data are due to the experiment and which are due to other factors during analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Contrast Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have created a `FirstLevelModel` object called `fmri_glm` and told it to fit our model. Now, we shall move onto null-hypothesis testing. \n",
    "\n",
    "Often in imaging studies, research questions can be expressed as null hypothesis tests of the below form:\n",
    "\n",
    "$$H_0: L\\beta^v = 0, \\quad H_1 : L\\beta^v \\neq 0$$\n",
    "    \n",
    "where $L$ is a fixed and known $(1 \\times p)$-sized contrast vector specifying a hypothesis, or prior belief, about linear relationships between the elements of $\\beta^v$, upon which an inference is to be made. Let's look at a few examples for `L`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "L_A = np.array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "L_R = np.array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have two contrast vectors; $L_{A}$ and $L_{R}$. Each contrast vector contains exactly one `1` and `0` everywhere else. \n",
    "\n",
    "By basic matrix multiplication, we can see that:\n",
    "\n",
    "$$L_{A}\\beta^v= \\begin{bmatrix} 1, & 0, & ..., & 0 \\end{bmatrix}\\begin{bmatrix}\\beta^v_{1}\\\\ \\beta^v_{2}\\\\ \\vdots \\\\ \\beta^v_{p}\\end{bmatrix}=\\beta^v_{1}$$\n",
    "\n",
    "where $\\beta^v_{i}$ is the $i^{th}$ element of $\\beta^{v}$. In other words, if we test the hypothesis that $L_{A}\\beta^v$ is equal to zero, we are in fact testing whether the first element of $\\beta^v$, $\\beta^v_{1}$, is zero.\n",
    "\n",
    "If we look back at our design matrix $X$, we can see that the first element of $\\beta^v$ corresponds to the `Active` column in the design matrix. This means that testing the hypothesis $L_{A}\\beta^v=0$ is really equivalent to asking something along the lines of \"did the participant being active in the task have a non-zero effect on their brain function (at voxel $v$)?\".\n",
    "\n",
    "Make sure you understand this before moving forward. If you have any questions feel free to ask one of the tutors! Check your understanding by trying to work out which hypothesis $L_R$ represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we are interested in more complex questions, like \"where was there a difference in brain activation between the subject responding to the task and being at rest?\". This can be represented using a contrast vector like so;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_active_minus_rest = L_A - L_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the contrast vector we shall be investigating in the following sections. To visualize the contrast vector `nilearn` has a nice feature called `plot_contrast_matrix`. Try it out below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_contrast_matrix\n",
    "plot_contrast_matrix(L_active_minus_rest, design_matrix=design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Effect Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to move onto creating images of our results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are testing to see whether there is statistical evidence that $L\\beta^v$ is non-zero at each voxel, it may of interest to look at an image of the values of $L\\hat{\\beta^v}$. We can do this using the `compute_contrast` function by setting the `output_type` to `effect_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_map = fmri_glm.compute_contrast(L_active_minus_rest,\n",
    "                                    output_type='effect_size')\n",
    "\n",
    "plot_stat_map(eff_map, bg_img=mean_img, vmax=10, cut_coords=(0,0,40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Note:** We'll be using the `plot_stat_map` a fair bit in the next sections, if you aren't sure what's happening at any point, check the documentation [here](https://nilearn.github.io/stable/modules/generated/nilearn.plotting.plot_stat_map.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice, we can sort-of see something here! But the above image doesn't really tell us anything rigorous... and it only tells us half the story! How do we know that we are seeing large values of $L\\hat{\\beta^v}$ by chance? We need to know something about the variance of $L\\hat{\\beta^v}$ as well! \n",
    "\n",
    "Fortunately, nilearn let's plot a map of $\\text{Var}(L\\hat{\\beta^v})$ as well by setting `output_type` to `effect_variance`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_map = fmri_glm.compute_contrast(L_active_minus_rest,\n",
    "                                    output_type='effect_variance')\n",
    "\n",
    "plot_stat_map(var_map, bg_img=mean_img,vmax=4, cut_coords=(0,0,40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good, as we can now see there is some pretty strong variance (in white) near the edge of the brain at the bottom of the image, suggesting we might not trust the estimates of $L\\beta^v$ in this region very much!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Mathematically the equation for the variance of $L\\hat{\\beta^v}$ is given by;\n",
    ">\n",
    "> $$\\text{Var}(L\\hat{\\beta^v})=L(X'[\\Sigma^v]^{-1}X)^{-1}L'$$\n",
    ">\n",
    "> However, in practice we don't know the value of $\\Sigma^v$ exactly, so this is approximated by $\\hat{\\Sigma}^v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this is very well and good but it's a bit handwavy. To really assess these results we need statistical tests..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing $Z$- and $t$-Statistic Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing in fMRI is most often performed using $t$-tests. The $t$-test incorporates the information present in both $L\\hat{\\beta^v}$ and our estimate of $\\text{Var}(L\\hat{\\beta^v})$ by first computing the $t$-statistic of the below form;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$t^v=\\frac{L\\hat{\\beta^v}}{\\sqrt{\\text{Var}(L\\hat{\\beta^v})}}=\\frac{L\\hat{\\beta^v}}{\\sqrt{L(X'[\\hat{\\Sigma}^v]^{-1}X)^{-1}L'}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try making an image of $t$-statistic values! We can do this with nilearn's `compute_contrast` by setting the `stat_type` to `t` and the `output_type` to `stat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_map = fmri_glm.compute_contrast(L_active_minus_rest,\n",
    "                                  stat_type='t',\n",
    "                                  output_type='stat')\n",
    "\n",
    "plot_stat_map(t_map, bg_img=mean_img,vmax=6, cut_coords=(0,0,40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the above is an image of our test statistic. To perform a statistical test we now need to convert this image into an image of $p$-values. Classical statistics tell us that $t^v$ should, in theory, be $T$-distributed under the null hypothesis $H_0$. This means that we can compute the $p$-values as follows (remember we are doing a two-tailed test):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p^v = 2\\times (1-\\mathbb{P}[T_{df}\\geq t^v|H_0])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In nilearn, we can create an image of $p$-values by setting the `stat_type` argument to `t` and the `output_type` argument to `p_value` in the `compute_contrast` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_map = fmri_glm.compute_contrast(L_active_minus_rest,\n",
    "                                  stat_type='t',\n",
    "                                  output_type='p_value')\n",
    "\n",
    "plot_stat_map(p_map, bg_img=mean_img, cut_coords=(0,0,40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** We have not mentioned what the degrees of freedom, $df$, of the $T$-distribution in question are here. This is because, in general, the computation used to calculate the degrees of freedom will vary depending on what structure is assumed for $\\Sigma^v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In imaging, it is also quite common practice to convert these p-values back to $Z$-scores using the below equation;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^v = \\Phi^{-1}\\bigg(1-\\frac{p^v}{2}\\bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\Phi$ is the cumulative distribution function for a standard gaussian random variable. This can be done in nilearn's `compute_contrast` by setting `output_type` to `z_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_map = fmri_glm.compute_contrast(L_active_minus_rest,\n",
    "                                  stat_type='t',\n",
    "                                  output_type='z_score')\n",
    "\n",
    "plot_stat_map(z_map, bg_img=mean_img, cut_coords=(0,0,40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason I have heard many cite for doing this is that it allows the results of studies to compared on a standard scale (i.e. if you and I have both run our own analysis with different degrees of freedom, we can still compare our results). Personally, I find this practice a little misleading, as you did not originally generate  𝑍 -scores but to each their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images we've generated so far are still a little messy and hard to interpret. In practice, we don't just want to subjectively try to assess the whole image, we want some form of \"objective\" (in quotation marks) visualisation which tells us where we have significant results. To do this we have to threshold our results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding your Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we say that we are \"thresholding an image\" what we really mean is that we have removed all the parts of the image where the values in the image are lower than some value we have deemed important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we could directly threshold the effect size image (i.e. the $L\\hat{\\beta^v}$ image) to see where exactly we have an effect size greater than $1$. In nilearn, we can view the thresholded image by using the `threshold` argument in the `plot_stat_map` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold our LB map\n",
    "plot_stat_map(eff_map, bg_img=mean_img, threshold=1,\n",
    "              display_mode='z', cut_coords=3, vmax=10, black_bg=True,\n",
    "              title='Active minus Rest (LB>1)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are just looking at all of the voxels where we had that $L\\hat{\\beta^v}>1$. There is some value to this image, as $L\\hat{\\beta^v}$ usually has a real-world interpretation of being the estimated change in [BOLD](https://en.wikipedia.org/wiki/Blood-oxygen-level-dependent_imaging) in fMRI. \n",
    "\n",
    "The above thresholded image is okay as a quick diagnostic to look at and may have some practical relevance, but it is hardly rigorous. As mentioned before, the $L\\hat{\\beta^v}$ image tells us our \"best-guess\" of the signal but it doesn't include any information about how reliable this guess is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To truly assess where in the brain we have seen evidence of activation, we must threshold the statistic values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nilearn does provide support for thresholding statistic values, but the mainly only for the $Z$-map.  \n",
    "\n",
    "> **Warning:** If you give the nilearn function `threshold_stats_img` a $t$-map nilearn will assume it is normal and not $T$-distributed.\n",
    "\n",
    "We can apply an uncorrected threshold of $\\alpha=5\\%$ to the $Z$-image in nilearn using the `threshold_stats_img` function by using the `fpr` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm import threshold_stats_img\n",
    "\n",
    "# Make a new threshold object\n",
    "_, threshold = threshold_stats_img(z_map, alpha=.05, height_control='fpr')\n",
    "\n",
    "# Print the threshold\n",
    "print('Uncorrected p<0.05 threshold: %.3f' % threshold)\n",
    "\n",
    "# Apply it to our statistic map\n",
    "plot_stat_map(z_map, bg_img=mean_img, threshold=threshold,\n",
    "              display_mode='z', cut_coords=3, black_bg=True,\n",
    "              title='Active minus Rest (p<0.05)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above image, only voxels at which the observed $Z$-values were significant at the $5\\%$ level have been retained. This is, of course **very bad practice!!**\n",
    "\n",
    "In fMRI, you must correct for [multiple comparisons](https://en.wikipedia.org/wiki/Multiple_comparisons_problem)!! \n",
    "\n",
    "In this image, we have already mentioned that there are aprroximately $87400$ voxels in the brain. This means that we have conducted about $87400$ independent statistical tests here (one for each voxel)! If the null hypothesis were true (e.g. if there were no difference brain activation at all between \"active\" and \"rest\" blocks) and we thresholded at the uncorrected $5\\%$ level, we would still expect $4370$ voxels ($=0.05\\times 87400$) to exceed an uncorrected threshold purely by chance! \n",
    "\n",
    "This would be terrible! How could we interpret our results when we know so many of the voxels showing in our thresholded image are false positives?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** You may be wondering why the uncorrected threshold is called $FPR$ in nilearn. $FPR$ here stands for False Positive Rate ($FPR$) and is given by:\n",
    ">\n",
    "> $$FPR = \\mathbb{E}\\bigg[\\frac{FP}{FP + TN}\\bigg]$$\n",
    "> \n",
    "> where $FP$ is the number of False Positives (the number of voxels incorrectly declared significant) and $TN$ is the number of true negatives (the number of voxels correctly declared non-significant). In other words, $FPR$ aims to control the expected proportion of voxels at which *the null hypothesis is true* which have been *incorrectly declared significant*.\n",
    "> \n",
    "> The reason that using $FPR$ is not a good idea for thresholding is that, if the null is true, the denominator of the expression above becomes the number of voxels in the brain. This is because, if the null is true at a voxel and a hypothesis test declares the voxel significant the voxel is counted towards $FP$, and, if the null is true at a voxel and a hypothesis test does not declare the voxel significant the voxel is counted towards $TN$. As a result, when we control $FDR$ all we are really doing is allowing $5\\%$ of our image to be declared significant under the null, which leads to a lot of false positives!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we threshold our data we must address the multiple comparisons problem. As seen above, $FPR$ did not do this. However, nilearn does provide other options which do address multiple comparisons. One such option is to control the [Family-Wise Error](https://en.wikipedia.org/wiki/Family-wise_error_rate) rate ($FWE$) using a [Bonferroni](https://en.wikipedia.org/wiki/Bonferroni_correction) correction. We can do this by setting `height_control` to `bonferroni`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new threshold object\n",
    "_, threshold = threshold_stats_img(\n",
    "    z_map, alpha=.05, height_control='bonferroni')\n",
    "\n",
    "# Print the threshold\n",
    "print('Bonferroni-corrected, p<0.05 threshold: %.3f' % threshold)\n",
    "\n",
    "# Apply it to our statistic map\n",
    "plot_stat_map(z_map, bg_img=mean_img, threshold=threshold,\n",
    "              display_mode='z', cut_coords=3, black_bg=True,\n",
    "              title='Active minus Rest (p<0.05, corrected)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Family-Wise Error rate is the probability of seeing at least one false positive under the null hypothesis. i.e.;\n",
    "\n",
    "$$FWE=\\mathbb{P}[FP\\geq 1| H_0]$$\n",
    "\n",
    "When we say that we are controlling the Family-Wise Error rate at the $\\alpha$ significance level, what we mean is we are ensuring that the above probability is less than or equal to $\\alpha$. The Bonferroni method is just one way of ensuring the above is less than or equal to $\\alpha$. \n",
    "\n",
    "The idea behind Bonferroni is very simple; we are worried that a threshold of $\\alpha$ (e.g. $\\alpha=0.05$) will give us too many false positives so, in order to counter this, we just divide $\\alpha$ by the number of tests we are conducting (e.g. $\\frac{\\alpha}{V}$, where $V$ is the number of voxels) and uses the result as a threshold instead.\n",
    "\n",
    "The Bonferonni method is very conservative, meaning it will tend to be overly cautious when declaring voxels significant. In the example, we have run above using Bonferroni we have essentially substituted the uncorrected threshold of $\\alpha=0.05$ for the corrected threshold of;\n",
    "\n",
    "$$\\frac{\\alpha}{V}= \\frac{0.05}{87400}\\approx 5.72\\times 10^{-7}$$\n",
    "\n",
    "This is an extremely small threshold and it can be viewed as overkill to use such a small significance value. As a result, many researchers prefer another popular alternative for addressing multiple comparisons; controlling the False Discovery Rate ($FDR$). We can control the False Discovery Rate with nilarn by setting `height_control` to `fdr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new threshold object\n",
    "_, threshold = threshold_stats_img(z_map, alpha=.05, height_control='fdr')\n",
    "\n",
    "# Print the threshold\n",
    "print('False Discovery rate = 0.05 threshold: %.3f' % threshold)\n",
    "\n",
    "# Apply it to our statistic map\n",
    "plot_stat_map(z_map, bg_img=mean_img, threshold=threshold,\n",
    "              display_mode='z', cut_coords=3, black_bg=True,\n",
    "              title='Active minus Rest (fdr=0.05)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The False Discovery Rate is the expected proportion of voxels at which *the test has been declared significant* that were *incorrectly declared significant*. i.e.;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$FDR = \\mathbb{E}\\bigg[\\frac{FP}{FP + TP}\\bigg]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $FP$ is the number of False Positives (the number of voxels incorrectly declared significant) and $TP$ is the number of true positives (the number of voxels correctly declared significant). Note that the denominator in the above differs from that used for $FPR$.\n",
    "\n",
    "When we say that we are controlling the False Discovery Rate at the $\\alpha$ significance level, what we mean is we are ensuring that the above expectation is less than or equal to $\\alpha$.\n",
    "\n",
    "As we can see above, the $FDR$ is a lot less conservative than $FWE$. \n",
    "\n",
    "You may at this point be thinking \"Which is the right way to threshold? Using $FWE$ or $FDR$?\". The, slightly frustrating, answer to this question is that there is no \"right\" answer to this question. $FDR$ and $FWE$ are simply controlling slightly different things. Both approaches are reasonable and often employed in the literature and there is no shortage of literature arguing either one is better. So long as you pick one and commit to it prior to thresholding your results (to avoid [p-hacking](https://en.wikipedia.org/wiki/Data_dredging)) you can't go too wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final option you can also employ is cluster-based thresholding. The above $FDR$ threshold gives you many significant regions, but it is quite clear that some of these regions are speckly noise, whilst others are genuine \"blobs\" of activity. To address this you can employ a cluster threshold like so;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new threshold object\n",
    "clean_map, threshold = threshold_stats_img(\n",
    "    z_map, alpha=.05, height_control='fdr', cluster_threshold=10)\n",
    "\n",
    "# Apply it to our statistic map\n",
    "plot_stat_map(clean_map, bg_img=mean_img, threshold=threshold,\n",
    "              display_mode='z', cut_coords=3, black_bg=True,\n",
    "              title='Active minus Rest (fdr=0.05), clusters > 10 voxels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above image, all \"blobs\" (or \"clusters\") which contained $10$ voxels or less have been removed from the image, leaving you with something a little more interpretable. \n",
    "\n",
    "You might think it seems a little arbitrary to just remove every cluster with $10$ voxels or less and, in truth, it is. In fMRI in general there are a lot of methods which decide the maximum size of cluster you want to keep in a more statistically sound manner (c.f. [Friston et al (1994)](https://pubmed.ncbi.nlm.nih.gov/24578041/)). However, you could fill up a whole other tutorial on that! If you want to know more about cluster extent thresholding ask one of the tutors (we both work in this area)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For years, it has been a common practice for imaging papers to share tables of values representing the voxels and clusters which remain after thresholding. As technology advances and it is becomes easier to share your data, this practice is starting to become more and more obsolete.\n",
    "\n",
    "However, you can still get a table of peaks and clusters in nilearn using the `get_clusters_table` function. You can check this out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.reporting import get_clusters_table\n",
    "table = get_clusters_table(z_map, stat_threshold=threshold,\n",
    "                           cluster_threshold=20)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you can understand what each of the columns in the above table means before moving on (you might have to look at the [documentation](https://nilearn.github.io/dev/modules/generated/nilearn.reporting.get_clusters_table.html) to help you).\n",
    "\n",
    "\n",
    "In this section, we have looked at many ways to threshold your data. Before we move on it is important that we give a general warning about repeatedly thresholding your data over and over again. If you were running a real analysis for a publication we strongly advise against repeatedly thresholding your data like we did above. Choose the threshold you want to use before looking at your results and stick to it.\n",
    "\n",
    "**Pick a threshold, and stick with it.**\n",
    "\n",
    "If you don't do this, you could, in theory, keep thresholding your data indefinitely until you get a result that is significant, or confirms your prior beliefs. This process introduces bias into your results and is known as [p-hacking](https://en.wikipedia.org/wiki/Data_dredging)! Don't do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $F$-Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last few sections, we have been looking at contrast vectors and statistics designed for null-hypothesis testing. Other statistical tests are of course possible; a popular one of which is the $F$-test.\n",
    "\n",
    "The $F$-test is a commonly employed method for assessing how good a fit your model gives when you include various parameters.\n",
    "\n",
    "For example, if we wished to assess how good our model fit is when we include only the elements of $\\beta$ which represent `Active` and `Rest`, we can stack the contrast vectors $L_A$ and $L_R$ into what is known as a contrast matrix, which we shall again denote $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a contrast matrix for F test\n",
    "L_F = np.vstack((L_A, L_R))\n",
    "\n",
    "# Show the contrast matrix we are using.\n",
    "plot_contrast_matrix(L_F, design_matrix)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $F$-test proceeds by calculating the $F$-statistic which takes the following form;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F^v=\\frac{\\hat{\\beta^v}'L'[L(X'[\\hat{\\Sigma}^v]^{-1}X)^{-1}L']^{-1}L\\hat{\\beta^v}}{rank(L)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate an $F$-statistic map in a similar manner to how we computed the $T$-statistic map earlier by using the `compute_contrast` function (this time setting `stat_type` to `F`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get F map\n",
    "f_map = fmri_glm.compute_contrast(L_F,\n",
    "                                  stat_type='F',\n",
    "                                  output_type='stat')\n",
    "\n",
    "# Plot the F map\n",
    "plot_stat_map(f_map, bg_img=mean_img,vmax=10, cut_coords=(0,0,40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same functions as before, we can create a $Z$-score image which corresponds to the $F$-test and threshold it. Try this out below;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You likely are seeing a lot of active regions around the ventricles and cortical boundaries. What do you think these results are telling you about your model? If you are unsure ask one the tutors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Return to where you first fit the linear model, with `FirstLevelModel` and add a smoothing option, e.g. `smoothing_fwhm=6`, to add a 6 mm FWHM smoothing kernel. Try a range of values for this and compare the images you get to one another. What effect does increasing the smoothing have on your results? Why do you think this is?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Investigate some other threshold options for the $F$-test above. Play around with the thresholding options a bit. What's giving you best results here and why? Be very careful with your answer\\*!! Explain carefully why in practice you wouldn't repeatedly threshold in this way.\n",
    "\n",
    "\\* This may be a bit of a trick question!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** In the next notebook, we will be looking at a second-level analysis. Conceptually, this means that each of our input images will correspond to a subject, rather than a timepoint in a timeseries. Before moving on to the next notebook, think a little about what you might have to do differently in a second-level analysis compared to a first-level analysis. Statistically speaking, what will change in your model, and why? \n",
    "\n",
    "Hint: Think about $\\Sigma$, drifts and the HRF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra:** If you raced through this tutorial and are massively ahead of time feel free to move onto the next notebook if you like. Alternatively, if you would like to see some more complex examples of first level analysis in `nilearn` check out the below notebooks from the nilearn website:\n",
    "\n",
    " - [Plot First Level Details](https://nilearn.github.io/stable/auto_examples/04_glm_first_level/plot_first_level_details.html)\n",
    " - [First Level Model](https://nilearn.github.io/stable/glm/first_level_model.html) \n",
    " - [GLM intro](https://nilearn.github.io/dev/glm/glm_intro.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
